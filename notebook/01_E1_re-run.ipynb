{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running Data\n",
    "\n",
    "- based on `rowid`s, there are 11920 records\n",
    "- GPT responses produced 11887 outputs\n",
    "- out of the 33 missing outputs. 10 were spot checked and found that also the rowid exists in VA and Age file, it is missing in open narrative file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The HEALSL project provides verbal autopsy data into multiple different files. Round one and round two; adult, child, and neonate; and questionnaire, \n",
    "age, and open narrative are all separate files. This script aims to combine these files into a single dataset (dataframe) to simplify processing.\n",
    "\n",
    "We extract the deceased's sex from the questionnaire dataset, the deceased's age from the age dataset, and the open narrative recorded from the\n",
    "verbal autopsy from the narrative dataset. Then, we combine the extracted features (columns) using their row id as the key.\n",
    "\n",
    "We utilize the OpenAI API's Chat Completions to generate a response for each verbal autopsy record. Several parameters were used for this project:\n",
    "message: the input text to be processed by the API. It consists of the combination of two text prompts: the system prompt, which is the same for \n",
    "all requests provides the model with some context of its role and objective, and the user prompt, which concatenates more specific instructions\n",
    "regarding the output response, along with the data from the dataframe.\n",
    "\n",
    "model: this parameter specifies the language model to be used. To strive for consistency and reproducibility of the results, we used specific versions\n",
    "of the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\n",
    "\n",
    "temperature: this parameter can be any value between 0 and 2. It controls the randomness of the output response. Low temperatures result in more \n",
    "deterministic responses, while high temperatures result in more random responses. We used a temperature of 0 to ensure the responses were as \n",
    "deterministic as possible.\n",
    "\n",
    "lobprobs: this parameters controls whether the output includes the log probabilities of the tokens. We set it to True to provide more parametric \n",
    "information about the output.\n",
    "\n",
    "The data files used are as follows:\n",
    "healsl_rd1_neo_v1.csv\n",
    "healsl_rd1_neo_age_v1.csv\n",
    "healsl_rd1_neo_narrative_v1.csv\n",
    "\n",
    "healsl_rd1_child_v1.csv\n",
    "healsl_rd1_child_age_v1.csv\n",
    "healsl_rd1_child_narrative_v1.csv\n",
    "\n",
    "healsl_rd1_adult_v1.csv\n",
    "healsl_rd1_adult_age_v1.csv\n",
    "healsl_rd1_adult_narrative_v1.csv\n",
    "\n",
    "healsl_rd2_neo_v1.csv\n",
    "healsl_rd2_neo_age_v1.csv\n",
    "healsl_rd2_neo_narrative_v1.csv\n",
    "\n",
    "healsl_rd2_child_v1.csv\n",
    "healsl_rd2_child_age_v1.csv\n",
    "healsl_rd2_child_narrative_v1.csv\n",
    "\n",
    "healsl_rd2_adult_v1.csv\n",
    "healsl_rd2_adult_age_v1.csv\n",
    "healsl_rd2_adult_narrative_v1.csv\n",
    "\n",
    "Note: This script only processes one age group (neo, child, adult) of one round (1 and 2) at each execution. Therefore, the user must manually change\n",
    "the input dataset filenames at execution to use the correct age group and round. The output filename can remain the same; as long as the rowids are\n",
    "unique, which is the case for the HEALSL datasets, the results for each rowid will only be written once in the output file.\n",
    "\n",
    "The prompts are as follows:\n",
    "\n",
    "System prompt:\n",
    "\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning ICD-10 codes for deaths using verbal autopsy \n",
    "narratives. Return only the ICD-10 code without description. E.g. A00 \n",
    "If there are multiple ICD-10 codes, show one code per line\"\n",
    "\n",
    "User prompt:\n",
    "\"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 code for a verbal autopsy narrative of a\n",
    "AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\n",
    "\n",
    "    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "    open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "The response from the API is then dissected to extract relevant information in plain text, such as the response text, the log probabilities, \n",
    "along with other accounting information such as rowids, models used, timestamps, token consumption, into an array and exported into file.\n",
    "        \n",
    "\n",
    "Pseudo code:\n",
    "1. Load the multiple datasets into respective dataframes.\n",
    "2. Extract the necessary features from each dataframe.\n",
    "3. Merge extracted features into a single dataframe using the rowid as the key.\n",
    "4. Load results storage as array.\n",
    "    If result storage is does not exist, create an empty array.\n",
    "5. For each row in the dataframe:\n",
    "    Check if rowid is in the result storage.\n",
    "        If rowid is in the result storage, skip the row.\n",
    "    Compose the two prompts and generate a response using the OpenAI API.\n",
    "    Store the response and other relevant information in the result storage.\n",
    "    Save the result storage to a file periodically.\n",
    "    \n",
    "---------\n",
    "\n",
    "1. Load datasets into dataframes D1, D2, ..., Dn.\n",
    "2. Extract necessary features F1, F2, ..., Fn from each dataframe.\n",
    "3. Merge features into a single dataframe D_merge using rowid as the key.\n",
    "4. Initialize results storage R as an empty array.\n",
    "5. For each row in D_merge:\n",
    "    a. If rowid is not in R:\n",
    "        i. Compose prompts and generate response using OpenAI API.\n",
    "        ii. Store response and relevant information in R.\n",
    "        iii. Periodically save R to a file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andyl\\AppData\\Local\\Temp\\ipykernel_40940\\1996176790.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "import datetime\n",
    "import pytz\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "open_api_key = os.environ.get('OPEN_API_KEY')\n",
    "client = OpenAI(api_key=open_api_key)\n",
    "\n",
    "WORDWRAP_WIDTH = 100\n",
    "DATA_FILE = \"data_storage.json\"\n",
    "SAVE_FREQ = 5\n",
    "\n",
    "# Models\n",
    "GPT4 = \"gpt-4-0613\"\n",
    "GPT3 = \"gpt-3.5-turbo-0125\"\n",
    "MODEL_NAME = GPT3\n",
    "\n",
    "# Set the timezone to Eastern Time\n",
    "TIMEZONE = pytz.timezone('US/Eastern')\n",
    "\n",
    "\n",
    "\n",
    "# SYS_PROMPT = \"\"\"You are a physician with expertise in determining underlying causes of death in Sierra Leone \n",
    "# by assigning ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code in JSON format: {“icd10”: [code1, code2, code3, code4, code5]}\"\"\"\n",
    "\n",
    "# SYS_PROMPT = \"\"\"You are a physician with expertise in determining underlying causes of death in Sierra Leone \n",
    "# by assigning ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code in JSON format: {“icd10”: code1}\"\"\"\n",
    "\n",
    "# USR_PROMPT = \"\"\"Determine the underlying cause of death and provide an ICD-10 code for a verbal autopsy narrative\n",
    "# of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\"\"\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. E.g. A00 \n",
    "If there are multiple ICD-10 codes, show one code per line.\"\"\"\n",
    "\n",
    "USR_PROMPT = \"\"\"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8320000000000003"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cost projection\n",
    "# ((((len(SYS_PROMPT + USR_PROMPT) // 4) + 300) / 1000) * 0.0005 + (15/1000) * 0.0015 ) * 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_df =  pd.read_csv(\"../data_202402/healsl_rd2_neo_v1.csv\")\n",
    "age_df =            pd.read_csv(\"../data_202402/healsl_rd2_neo_age_v1.csv\")\n",
    "narrative_df =      pd.read_csv(\"../data_202402/healsl_rd2_neo_narrative_v1.csv\")\n",
    "\n",
    "narrative_df = narrative_df.rename(columns={'summary': 'open_narrative'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_gp3 = pd.read_csv(\"../data_202402/healsl_rd1_rapid_chatgpt3_v1.csv\")\n",
    "# quick_gp4 = pd.read_csv(\"../data_202402/healsl_rd1_rapid_chatgpt4_v1.csv\")\n",
    "\n",
    "# questionnaire_df[questionnaire_df['p1_recon_icd_cod'].isna()][['rowid','p1_icd_cod','p2_icd_cod']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of merged_df (233, 5):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rowid</th>\n",
       "      <th>open_narrative</th>\n",
       "      <th>sex_cod</th>\n",
       "      <th>age_value_death</th>\n",
       "      <th>age_unit_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>24000243</td>\n",
       "      <td>As per respondent, the child was fresh still b...</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>24003291</td>\n",
       "      <td>The deceased was 0 day male who was a still bi...</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>24000616</td>\n",
       "      <td>According to the respondent, the deceased was ...</td>\n",
       "      <td>Male</td>\n",
       "      <td>7</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>24000116</td>\n",
       "      <td>According to the respondent the deceased was a...</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24003613</td>\n",
       "      <td>As per respondent, the decease was a 0D old bo...</td>\n",
       "      <td>Male</td>\n",
       "      <td>10</td>\n",
       "      <td>Days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rowid                                     open_narrative sex_cod  \\\n",
       "48   24000243  As per respondent, the child was fresh still b...  Female   \n",
       "195  24003291  The deceased was 0 day male who was a still bi...    Male   \n",
       "178  24000616  According to the respondent, the deceased was ...    Male   \n",
       "74   24000116  According to the respondent the deceased was a...  Female   \n",
       "16   24003613  As per respondent, the decease was a 0D old bo...    Male   \n",
       "\n",
       "     age_value_death age_unit_death  \n",
       "48                 0           Days  \n",
       "195                0           Days  \n",
       "178                7           Days  \n",
       "74                 2           Days  \n",
       "16                10           Days  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "narrative_only = narrative_df[['rowid','open_narrative']]\n",
    "sex_only = questionnaire_df[['rowid','sex_cod']]\n",
    "age_only = age_df[['rowid','age_value_death','age_unit_death']]\n",
    "\n",
    "merged_df = narrative_only.merge(sex_only, on='rowid').merge(age_only, on='rowid')\n",
    "\n",
    "# Fill in missing values with empty string\n",
    "merged_df['sex_cod'] = merged_df['sex_cod'].fillna('')\n",
    "\n",
    "assert not merged_df.isnull().values.any(), \"Execution halted: NaN values found in merged_df\"\n",
    "\n",
    "print(f\"Sample of merged_df {merged_df.shape}:\")\n",
    "display(merged_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Initialize the data storage dictionary\n",
    "\n",
    "def load_data(filename=DATA_FILE):\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} found. Loading data...\")\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"{filename} not found. Initializing empty dictionary...\")\n",
    "        return {}\n",
    "\n",
    "def save_data(data, filename=DATA_FILE):\n",
    "    # Save data to a file    \n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Send a message to the chatbot\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # max_tokens=500,\n",
    "    temperature=0,\n",
    "    # stop=None,\n",
    "    # seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        # \"response_format\": { \"type\": \"json_object\" },\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        # \"stop\": stop,\n",
    "        # \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_storage.json found. Loading data...\n",
      "\n",
      "Saving index: 232      Processing: 24001069     Rows skipped: 0\n",
      "Data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "# Load existing data or initialize an empty dictionary\n",
    "data_storage = load_data()\n",
    "skipped_rows = []\n",
    "repeated_skips = False\n",
    "print()\n",
    "\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Access the values of each column in the current row\n",
    "    # hijacking row \n",
    "    # row = merged_df[merged_df['rowid'] == 14005966].iloc[0]\n",
    "    \n",
    "    rowid = row['rowid']\n",
    "    \n",
    "    # Check if rowid already processed. Testing both because json changes int keys to str    \n",
    "    if (rowid) in data_storage or str(rowid) in data_storage:\n",
    "        if repeated_skips:\n",
    "            print(\"\\r\", end='', flush=True)\n",
    "        print(f\"Skipping index {index}, row {rowid} - Already processed.\", end='', flush=True)\n",
    "        repeated_skips = True\n",
    "        skipped_rows.append(rowid)\n",
    "        continue\n",
    "\n",
    "    \n",
    "    narrative = row['open_narrative']\n",
    "    sex_cod = row['sex_cod']\n",
    "    age_value_death = row['age_value_death']\n",
    "    age_unit_death = row['age_unit_death']\n",
    "    \n",
    "    prompt = USR_PROMPT\n",
    "    prompt = prompt.replace('AGE_VALUE_DEATH', str(age_value_death))\n",
    "    prompt = prompt.replace('AGE_UNIT_DEATH', age_unit_death.lower())\n",
    "    prompt = prompt.replace('SEX_COD', sex_cod.lower())\n",
    "    prompt = prompt.format(open_narrative=narrative)\n",
    "    \n",
    "    # print(\"Prompt:\")    \n",
    "    # print(textwrap.fill(prompt, width=WORDWRAP_WIDTH))\n",
    "    # print()\n",
    "    \n",
    "    # for a in range(5):\n",
    "    completion = get_completion(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] ,\n",
    "        model=MODEL_NAME,\n",
    "        logprobs=True,\n",
    "        # top_logprobs=2,\n",
    "    )\n",
    "    \n",
    "    # print(completion.choices[0].message)\n",
    "    \n",
    "    # for token in completion.choices[0].logprobs.content:\n",
    "    #     print(f\"{repr(str(token.token)).ljust(15)}  {str(token.logprob).ljust(20)} {np.round(np.exp(token.logprob)*100,2)}%\")\n",
    "        \n",
    "    output_msg = completion.choices[0].message.content\n",
    "    logprob_data = [(token.token, float(token.logprob)) for token in completion.choices[0].logprobs.content]\n",
    "    usage_data = list(completion.usage)    \n",
    "    current_time = datetime.datetime.now(tz=TIMEZONE).isoformat()\n",
    "       \n",
    "    data_storage[str(rowid)] = {\n",
    "        'rowid': rowid,\n",
    "        'model': MODEL_NAME,\n",
    "        'system_prompt': SYS_PROMPT,\n",
    "        'user_prompt': prompt,\n",
    "        'output_msg': output_msg,\n",
    "        'logprobs': logprob_data,\n",
    "        'usage': usage_data,\n",
    "        'timestamp': current_time\n",
    "    }\n",
    "\n",
    "    # Save data periodically (you can adjust the frequency based on your needs)    \n",
    "    if index % SAVE_FREQ == 0 and index > 0:\n",
    "        if repeated_skips:\n",
    "            print(\"\\n\", flush=True)\n",
    "        repeated_skips = False\n",
    "        \n",
    "        save_data(data_storage)\n",
    "        print(f\"Saving index: {str(index).ljust(8)} Processing: {str(rowid).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "        # break\n",
    "    \n",
    "try:\n",
    "    save_data(data_storage)\n",
    "    print(f\"Saving index: {str(index).ljust(8)} Processing: {str(rowid).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "    print(\"\\nData saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "if len(skipped_rows) > 0:\n",
    "    print(f\"DF length: {len(merged_df)}\")\n",
    "    print(f\"Rows skipped: {len(skipped_rows)}\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
