03-13

- Ran GPT4 for 11.5 hours
- decide to set max tokens to 40 moving forward
- decide to export "first ICD" and "sorted ICD" moving forward
- reprocess both gpt3 and gpt4

03-14

- create a new directory just for working exports
- new files needed
- found bug in calculating logprobs. added strip to remove \n from in df

all data
1. compiled into format usable for openAI api
2. process using api -> export JSON files
3. extract ICD codes -> export (1) sorted_icd.csv (2) first_icd.csv
4. split by round

sampling data
1. 

THOUGHTS:
- when using max_token, we truncate the last output. There's a risk that the last incomplete yet valid syntax icd10 code produces returns returns the best score
- i suppose if there are more than 5 ICD, drop the last one.

03-15
- finished running all gpt4 experiments

03-18
- compiling another set of outputs using first ICD10 code from output
- determine that first ICD10 code yields better PCCC

03-19
- dropping sorted ICD10 moving forward, removed cause(x)_probs as it is no longer needed
- fill in meta and datadict files

Changing file structure, ready for publication
1. preprocessing 
2. run API
3. extract ICD10
4a. for full - split to round1/2
4b. for sample - aggregate 1x..10x

merge [1 > 2 > 3]
full 1 > 2 > 3 > 4a
sample 1 > 2 > 3 > 4b

Step A = full
step b = sample
lower case for the filenames