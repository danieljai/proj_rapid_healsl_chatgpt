{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script loads the dataset compiled from the previous step and generate a responses from OpenAI\\'s API for each record.\\n\\nPseudo code\\n-----------\\n1. Load results storage as array.\\n    1.1 If result storage is does not exist, create an empty array.\\n2. For each row in the dataframe:\\n    2.1 Check if rowid is in the result storage.\\n        2.1.1 If exists: record, skip the row.\\n    2.2 Compose the two prompts and generate a response using the OpenAI API.\\n    2.3 Store the response and other relevant information in the result storage.\\n    2.4 Save the result storage to a file periodically.\\n3. Save the result storage one last time.\\n    3.1 If rows were skipped, print a warning message.\\n    3.2 save the skipped rows to a file.\\n\\n\\nDetails regarding #2.1.1 of the pseudo code:\\n--------------------------------------------\\n\\nSince responses are billed by token consumption, we want to avoid reprocessing the same record. Previously processed \\nare stored and saved using the unique identifier as key, and when the same storage file is loaded, the script checks\\nwhether the unique identifier is already in the storage.\\n\\nFor accounting purposes, we store any unique identifier that was skipped in a separate file. in #3.2.\\n\\n    \\nDetails regarding #2.2 of the pseudo code:\\n------------------------------------------\\n\\nWe utilize the OpenAI API\\'s Chat Completions to generate a response from the model. The parameters used are as follows:\\n\\n    message: This is input text to be processed by the API. It is composed of two text prompts: \\n        System Prompt: Provides the model context on its role and the expected output format. Same for all records. \\n            We used the following system prompt:\\n            \"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning \\n            ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. \\n            E.g. A00 If there are multiple ICD-10 codes, show one code per line\"\\n            \\n        User Prompt: Specific instructions and individual record data.\\n            We used the following user prompt:\\n            \"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 \\n            code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: \\n            {open_narrative}\"\\n            \\n            AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\\n            SEX_COD: replaced with sex_cod value from the questionnaire dataset.\\n            open_narrative: replaced with summary value from the open narrative dataset.\\n\\n    model: This parameter specifies the language model to be used. To strive for consistency and reproducibility of the \\n    results, we used specific versions of the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\\n    \\n    temperature: This parameter can be any value between 0 and 2. Low temperatures result in more deterministic responses, \\n    while high temperatures result in more random responses. We set this to 0.\\n\\n    lobprobs: This parameters controls whether the output includes the log probabilities of the tokens. We set it to True.\\n\\n    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\\n    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\\n    open_narrative: replaced with summary value from the open narrative dataset.\\n\\n\\nDetails regarding #2.3 of the pseudo code:\\n------------------------------------------\\n\\nBelow is the data structure of the output data:\\n\\n    record[uid] = {\\n        \\'uid\\': the unique identifier for the dataset  --> change to uid\\n        \\'rowid\\': original rowid,\\n        \\'param_model\\': model used\\n        \\'param_temperature\\': temperature used\\n        \\'param_logprobs\\': logprobs used\\n        \\'param_system_prompt\\': system prompt used\\n        \\'param_user_prompt\\': user prompt used\\n        \\'output_msg\\': output text message from the API // drop\\n        \\'output_logprobs\\': the log probabilities of the tokens in the output message // drop\\n        \\'output_usage\\': token consumption // drop\\n        \\'output_timestamp\\': current timestamp\\n        \\'other_columns\\': extra columns that were not recognized by the script. Can be useful for debugging or incorporating additional information.\\n        \\'output\\': serialized response from the Chat Completions API\\n    }\\n    \\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script loads the dataset compiled from the previous step and generate a responses from OpenAI's API for each record.\n",
    "\n",
    "Pseudo code\n",
    "-----------\n",
    "1. Load results storage as array.\n",
    "    1.1 If result storage is does not exist, create an empty array.\n",
    "2. For each row in the dataframe:\n",
    "    2.1 Check if rowid is in the result storage.\n",
    "        2.1.1 If exists: record, skip the row.\n",
    "    2.2 Compose the two prompts and generate a response using the OpenAI API.\n",
    "    2.3 Store the response and other relevant information in the result storage.\n",
    "    2.4 Save the result storage to a file periodically.\n",
    "3. Save the result storage one last time.\n",
    "    3.1 If rows were skipped, print a warning message.\n",
    "    3.2 save the skipped rows to a file.\n",
    "\n",
    "\n",
    "Details regarding #2.1.1 of the pseudo code:\n",
    "--------------------------------------------\n",
    "\n",
    "Since responses are billed by token consumption, we want to avoid reprocessing the same record. Previously processed \n",
    "are stored and saved using the unique identifier as key, and when the same storage file is loaded, the script checks\n",
    "whether the unique identifier is already in the storage.\n",
    "\n",
    "For accounting purposes, we store any unique identifier that was skipped in a separate file. in #3.2.\n",
    "\n",
    "    \n",
    "Details regarding #2.2 of the pseudo code:\n",
    "------------------------------------------\n",
    "\n",
    "We utilize the OpenAI API's Chat Completions to generate a response from the model. The parameters used are as follows:\n",
    "\n",
    "    message: This is input text to be processed by the API. It is composed of two text prompts: \n",
    "        System Prompt: Provides the model context on its role and the expected output format. Same for all records. \n",
    "            We used the following system prompt:\n",
    "            \"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning \n",
    "            ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. \n",
    "            E.g. A00 If there are multiple ICD-10 codes, show one code per line\"\n",
    "            \n",
    "        User Prompt: Specific instructions and individual record data.\n",
    "            We used the following user prompt:\n",
    "            \"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 \n",
    "            code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: \n",
    "            {open_narrative}\"\n",
    "            \n",
    "            AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "            SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "            open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "    model: This parameter specifies the language model to be used. To strive for consistency and reproducibility of the \n",
    "    results, we used specific versions of the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\n",
    "    \n",
    "    temperature: This parameter can be any value between 0 and 2. Low temperatures result in more deterministic responses, \n",
    "    while high temperatures result in more random responses. We set this to 0.\n",
    "\n",
    "    lobprobs: This parameters controls whether the output includes the log probabilities of the tokens. We set it to True.\n",
    "\n",
    "    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "    open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "\n",
    "Details regarding #2.3 of the pseudo code:\n",
    "------------------------------------------\n",
    "\n",
    "Below is the data structure of the output data:\n",
    "\n",
    "    record[uid] = {\n",
    "        'uid': the unique identifier for the dataset  --> change to uid\n",
    "        'rowid': original rowid,\n",
    "        'param_model': model used\n",
    "        'param_temperature': temperature used\n",
    "        'param_logprobs': logprobs used\n",
    "        'param_system_prompt': system prompt used\n",
    "        'param_user_prompt': user prompt used\n",
    "        # 'output_msg': output text message from the API // drop\n",
    "        # 'output_logprobs': the log probabilities of the tokens in the output message // drop\n",
    "        # 'output_usage': token consumption // drop\n",
    "        'output_timestamp': current timestamp\n",
    "        # 'other_columns': extra columns that were not recognized by the script. Can be useful for debugging or incorporating additional information.\n",
    "        'output': serialized response from the Chat Completions API\n",
    "        ... : passthough columns not required by the script but included in the original dataset\n",
    "    }\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "import datetime\n",
    "import pytz\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "open_api_key = os.environ.get('OPEN_API_KEY')\n",
    "client = OpenAI(api_key=open_api_key)\n",
    "\n",
    "# If set to True, the script will only process a subset of the data\n",
    "DEMO_MODE = True\n",
    "\n",
    "# Discard columns that are not recognized by the script in the output file.\n",
    "# Keep this to False if you want the output file to retain columns needed for post-processing purposes.\n",
    "DISCARD_EXCESS_COLUMNS = False\n",
    "\n",
    "# Input datasets\n",
    "INPUT_DATASET_FILE = \"healsl_dataset_all.csv\"\n",
    "\n",
    "# Output file\n",
    "OUTPUT_DATA_FILE = \"testing_response_mac.json\"\n",
    "\n",
    "\n",
    "# Set the timezone to Eastern Time\n",
    "TIMEZONE = pytz.timezone('US/Eastern')\n",
    "\n",
    "WORDWRAP_WIDTH = 100\n",
    "\n",
    "# How often the temp storage is saved to disk\n",
    "SAVE_FREQ = 5\n",
    "\n",
    "# Models\n",
    "GPT4 = \"gpt-4-0613\"\n",
    "GPT3 = \"gpt-3.5-turbo-0125\"\n",
    "MODEL_NAME = GPT3\n",
    "TEMPERATURE = 0\n",
    "LOGPROBS = True\n",
    "\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. E.g. A00 \n",
    "If there are multiple ICD-10 codes, show one code per line.\"\"\"\n",
    "\n",
    "USR_PROMPT = \"\"\"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_passed = True\n",
    "required_colnames = ['uid', 'rowid', 'age_value_death', 'age_unit_death',\n",
    "                     'open_narrative', 'sex_cod']\n",
    "\n",
    "# Load dataset\n",
    "merged_df = pd.read_csv(INPUT_DATASET_FILE)\n",
    "\n",
    "# # TEST: making up a new column\n",
    "merged_df = merged_df.assign(other_2=\"extra stuff\")\n",
    "\n",
    "# Check all required columns are in the df\n",
    "for colnames in required_colnames:\n",
    "    if colnames not in merged_df.columns:\n",
    "        print(f\"Missing column \\\"{colnames}\\\"\")\n",
    "        dataset_passed = False\n",
    "\n",
    "if not dataset_passed:\n",
    "    print(\"Please ensure dataset has all the required columns.\")\n",
    "    raise ValueError(f\"Error: Missing columns required for processing.\")\n",
    "    \n",
    "# Get columns names that are not required\n",
    "extra_colnames = ['uid']\n",
    "extra_colnames += [colname for colname in merged_df.columns if colname not in required_colnames]\n",
    "\n",
    "# Transform non-required columns as dictionary in a new column\n",
    "merged_df['model_residual_columns'] = merged_df[extra_colnames].apply(lambda x: x.to_dict(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMO MODE: Processing only 25 records.\n"
     ]
    }
   ],
   "source": [
    "# When DEMO_MODE is set to True, process only a subset of the data\n",
    "if DEMO_MODE:\n",
    "    limit_records = int(min(merged_df.shape[0] * 0.01, 5))\n",
    "    print(f\"DEMO MODE: Processing only {limit_records} records.\")\n",
    "    merged_df = merged_df.sample(limit_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>rowid</th>\n",
       "      <th>age</th>\n",
       "      <th>group</th>\n",
       "      <th>other_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14003542</th>\n",
       "      <td>14003542</td>\n",
       "      <td>14003542</td>\n",
       "      <td>30</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24003194</th>\n",
       "      <td>24003194</td>\n",
       "      <td>24003194</td>\n",
       "      <td>2</td>\n",
       "      <td>child_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006115</th>\n",
       "      <td>14006115</td>\n",
       "      <td>14006115</td>\n",
       "      <td>2</td>\n",
       "      <td>child_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000020</th>\n",
       "      <td>24000020</td>\n",
       "      <td>24000020</td>\n",
       "      <td>19</td>\n",
       "      <td>adult_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002449</th>\n",
       "      <td>24002449</td>\n",
       "      <td>24002449</td>\n",
       "      <td>1</td>\n",
       "      <td>child_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14005134</th>\n",
       "      <td>14005134</td>\n",
       "      <td>14005134</td>\n",
       "      <td>40</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006846</th>\n",
       "      <td>14006846</td>\n",
       "      <td>14006846</td>\n",
       "      <td>5</td>\n",
       "      <td>child_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14002113</th>\n",
       "      <td>14002113</td>\n",
       "      <td>14002113</td>\n",
       "      <td>35</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14006870</th>\n",
       "      <td>14006870</td>\n",
       "      <td>14006870</td>\n",
       "      <td>5</td>\n",
       "      <td>child_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14001175</th>\n",
       "      <td>14001175</td>\n",
       "      <td>14001175</td>\n",
       "      <td>45</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14004603</th>\n",
       "      <td>14004603</td>\n",
       "      <td>14004603</td>\n",
       "      <td>0</td>\n",
       "      <td>neo_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000988</th>\n",
       "      <td>24000988</td>\n",
       "      <td>24000988</td>\n",
       "      <td>98</td>\n",
       "      <td>adult_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002952</th>\n",
       "      <td>24002952</td>\n",
       "      <td>24002952</td>\n",
       "      <td>60</td>\n",
       "      <td>adult_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002626</th>\n",
       "      <td>24002626</td>\n",
       "      <td>24002626</td>\n",
       "      <td>70</td>\n",
       "      <td>adult_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14007293</th>\n",
       "      <td>14007293</td>\n",
       "      <td>14007293</td>\n",
       "      <td>55</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14005440</th>\n",
       "      <td>14005440</td>\n",
       "      <td>14005440</td>\n",
       "      <td>28</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24002326</th>\n",
       "      <td>24002326</td>\n",
       "      <td>24002326</td>\n",
       "      <td>70</td>\n",
       "      <td>adult_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14003658</th>\n",
       "      <td>14003658</td>\n",
       "      <td>14003658</td>\n",
       "      <td>2</td>\n",
       "      <td>child_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14004861</th>\n",
       "      <td>14004861</td>\n",
       "      <td>14004861</td>\n",
       "      <td>55</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14004235</th>\n",
       "      <td>14004235</td>\n",
       "      <td>14004235</td>\n",
       "      <td>64</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14004282</th>\n",
       "      <td>14004282</td>\n",
       "      <td>14004282</td>\n",
       "      <td>55</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24001005</th>\n",
       "      <td>24001005</td>\n",
       "      <td>24001005</td>\n",
       "      <td>8</td>\n",
       "      <td>child_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14003193</th>\n",
       "      <td>14003193</td>\n",
       "      <td>14003193</td>\n",
       "      <td>0</td>\n",
       "      <td>neo_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000180</th>\n",
       "      <td>14000180</td>\n",
       "      <td>14000180</td>\n",
       "      <td>0</td>\n",
       "      <td>neo_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14001184</th>\n",
       "      <td>14001184</td>\n",
       "      <td>14001184</td>\n",
       "      <td>40</td>\n",
       "      <td>adult_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               uid     rowid age      group      other_2\n",
       "14003542  14003542  14003542  30  adult_rd1  extra stuff\n",
       "24003194  24003194  24003194   2  child_rd2  extra stuff\n",
       "14006115  14006115  14006115   2  child_rd1  extra stuff\n",
       "24000020  24000020  24000020  19  adult_rd2  extra stuff\n",
       "24002449  24002449  24002449   1  child_rd2  extra stuff\n",
       "14005134  14005134  14005134  40  adult_rd1  extra stuff\n",
       "14006846  14006846  14006846   5  child_rd1  extra stuff\n",
       "14002113  14002113  14002113  35  adult_rd1  extra stuff\n",
       "14006870  14006870  14006870   5  child_rd1  extra stuff\n",
       "14001175  14001175  14001175  45  adult_rd1  extra stuff\n",
       "14004603  14004603  14004603   0    neo_rd1  extra stuff\n",
       "24000988  24000988  24000988  98  adult_rd2  extra stuff\n",
       "24002952  24002952  24002952  60  adult_rd2  extra stuff\n",
       "24002626  24002626  24002626  70  adult_rd2  extra stuff\n",
       "14007293  14007293  14007293  55  adult_rd1  extra stuff\n",
       "14005440  14005440  14005440  28  adult_rd1  extra stuff\n",
       "24002326  24002326  24002326  70  adult_rd2  extra stuff\n",
       "14003658  14003658  14003658   2  child_rd1  extra stuff\n",
       "14004861  14004861  14004861  55  adult_rd1  extra stuff\n",
       "14004235  14004235  14004235  64  adult_rd1  extra stuff\n",
       "14004282  14004282  14004282  55  adult_rd1  extra stuff\n",
       "24001005  24001005  24001005   8  child_rd2  extra stuff\n",
       "14003193  14003193  14003193   0    neo_rd1  extra stuff\n",
       "14000180  14000180  14000180   0    neo_rd1  extra stuff\n",
       "14001184  14001184  14001184  40  adult_rd1  extra stuff"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POC\n",
    "\n",
    "temp = {}\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    uid = row['uid'] \n",
    "    rowid = row['rowid']\n",
    "\n",
    "    age_value = row['age_value_death']\n",
    "\n",
    "    # print(row[extra_colnames].to_dict())\n",
    "\n",
    "    temp[str(uid)] = {\n",
    "        'uid': uid,               # 'uid' is the unique identifier for the dataset\n",
    "        'rowid': rowid,\n",
    "        'age': age_value,\n",
    "        # 'other_columns': other_columns,\n",
    "    }\n",
    "\n",
    "    temp[str(uid)].update(row[extra_colnames].to_dict())\n",
    "\n",
    "    # break\n",
    "\n",
    "pd.DataFrame(temp).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Misc. functions\n",
    "\n",
    "# Function to get current time in string YYMMDD_HHMMSS format\n",
    "def get_current_str_time():\n",
    "    return datetime.datetime.now(tz=TIMEZONE).strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "# Used to convert ChatCompletion object to a dictionary, recursively\n",
    "def recursive_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursive_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_dict(v) for v in obj]\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return recursive_dict(obj.__dict__)\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Initialize the data storage dictionary\n",
    "\n",
    "def load_data(filename=OUTPUT_DATA_FILE):\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} found. Loading data...\")\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"{filename} not found. Initializing empty dictionary...\")\n",
    "        return {}\n",
    "\n",
    "def save_data(data, filename=OUTPUT_DATA_FILE):           \n",
    "    # Save data to a file   \n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Send a message to the chatbot\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # max_tokens=500,\n",
    "    temperature=0,\n",
    "    # stop=None,\n",
    "    # seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        # \"response_format\": { \"type\": \"json_object\" },\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        # \"stop\": stop,\n",
    "        # \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_response_mac.json not found. Initializing empty dictionary...\n",
      "\n",
      "Saving index: 24       Processing: 14007954     Rows skipped: 0\n",
      "Data saved successfully. Processing Complete.\n"
     ]
    }
   ],
   "source": [
    "# Load existing data or initialize an empty dictionary\n",
    "data_storage = load_data()\n",
    "skipped_rows = []\n",
    "repeated_skips = False\n",
    "print()\n",
    "\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Access the values of each column in the current row\n",
    "    # hijacking row \n",
    "    # row = merged_df[merged_df['rowid'] == 14005966].iloc[0]\n",
    "    \n",
    "    uid = row['uid']    \n",
    "    \n",
    "    # Check if rowid already processed. Testing both because json changes int keys to str    \n",
    "    if (uid) in data_storage or str(uid) in data_storage:\n",
    "        # if repeated_skips:\n",
    "        #     print(\"\\r\", end='', flush=True)\n",
    "        # print(f\"Skipping index {index}, row {uid} - Already processed.\", end='', flush=True)\n",
    "        repeated_skips = True\n",
    "        skipped_rows.append(uid)\n",
    "        continue\n",
    "\n",
    "    rowid = row['rowid']\n",
    "    narrative = row['open_narrative']\n",
    "    sex_cod = row['sex_cod']\n",
    "    age_value_death = row['age_value_death']\n",
    "    age_unit_death = row['age_unit_death']\n",
    "    other_columns = row[extra_colnames].to_dict()\n",
    "    \n",
    "    prompt = USR_PROMPT\n",
    "    prompt = prompt.replace('AGE_VALUE_DEATH', str(age_value_death))\n",
    "    prompt = prompt.replace('AGE_UNIT_DEATH', age_unit_death.lower())\n",
    "    prompt = prompt.replace('SEX_COD', sex_cod.lower())\n",
    "    prompt = prompt.format(open_narrative=narrative)\n",
    "    \n",
    "    # print(\"Prompt:\")    \n",
    "    # print(textwrap.fill(prompt, width=WORDWRAP_WIDTH))\n",
    "    # print()\n",
    "    \n",
    "    # for a in range(5):\n",
    "    completion = get_completion(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] ,\n",
    "        model=MODEL_NAME,\n",
    "        logprobs=LOGPROBS,\n",
    "        temperature=TEMPERATURE,\n",
    "        # top_logprobs=2,\n",
    "    )\n",
    "    \n",
    "    # print(completion.choices[0].message)\n",
    "    \n",
    "    # for token in completion.choices[0].logprobs.content:\n",
    "    #     print(f\"{repr(str(token.token)).ljust(15)}  {str(token.logprob).ljust(20)} {np.round(np.exp(token.logprob)*100,2)}%\")\n",
    "        \n",
    "    output_msg = completion.choices[0].message.content\n",
    "    logprob_data = [(token.token, float(token.logprob)) for token in completion.choices[0].logprobs.content]\n",
    "    usage_data = list(completion.usage)    \n",
    "    current_time = datetime.datetime.now(tz=TIMEZONE).isoformat()\n",
    "       \n",
    "    data_storage[str(uid)] = {\n",
    "        'uid': uid,               # 'uid' is the unique identifier for the dataset\n",
    "        'rowid': rowid,\n",
    "        'param_model': MODEL_NAME,\n",
    "        'param_temperature': 0,\n",
    "        'param_logprobs': True,\n",
    "        'param_system_prompt': SYS_PROMPT,\n",
    "        'param_user_prompt': prompt,\n",
    "        # 'output_msg': output_msg,\n",
    "        # 'logprobs': logprob_data,\n",
    "        # 'usage': usage_data,\n",
    "        'timestamp': current_time,\n",
    "        # 'other_columns': other_columns,\n",
    "        'output': recursive_dict(completion),\n",
    "    }\n",
    "\n",
    "    if not DISCARD_EXCESS_COLUMNS:\n",
    "        # Append columns not required by the script but exists on the original dataset\n",
    "        data_storage[str(uid)].update(other_columns)\n",
    "\n",
    "    # Save data periodically (you can adjust the frequency based on your needs)    \n",
    "    if index % SAVE_FREQ == 0 and index > 0:\n",
    "        if repeated_skips:\n",
    "            print(\"\\n\", flush=True)\n",
    "        repeated_skips = False\n",
    "        \n",
    "        save_data(data_storage)\n",
    "        print(f\"Saving index: {str(index).ljust(8)} Processing: {str(uid).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "        # break\n",
    "    \n",
    "try:\n",
    "    save_data(data_storage)\n",
    "    print(f\"Saving index: {str(index).ljust(8)} Processing: {str(uid).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "    print(\"\\nData saved successfully. Processing Complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "if len(skipped_rows) > 0:\n",
    "    print(f\"{len(skipped_rows)} rows skipped. Check skipped_rows for details.\")\n",
    "    \n",
    "    # Write skipped rows to a file\n",
    "    with open(f\"response_report_{get_current_str_time()}.txt\", \"w\") as file:\n",
    "        file.write(f\"The follow rows are skipped because they were already processed.\\n\")\n",
    "        for item in skipped_rows:        \n",
    "            file.write(f\"{str(item)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_storage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
