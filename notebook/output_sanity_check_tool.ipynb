{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "filename = './healsl_rd2_rapid_gpt4_v2b_sorted_ICD.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "df.output_probs = df.output_probs.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gpt3\" in filename:\n",
    "    model_name = \"GPT3\"\n",
    "elif \"gpt4\"in filename:\n",
    "    model_name = \"GPT4\"\n",
    "\n",
    "if \"first_ICD\" in filename:\n",
    "    sort_scheme = \"First ICD\"\n",
    "    test_sort = False\n",
    "elif \"sorted_ICD\"  in filename:\n",
    "    sort_scheme = \"Highest Prob\"\n",
    "    test_sort = True\n",
    "    \n",
    "if \"rd1\" in filename:\n",
    "    round_num = \"Round 1\"\n",
    "elif \"rd2\" in filename:\n",
    "    round_num = \"Round 2\"\n",
    "    \n",
    "if \"sample\" in filename:\n",
    "    sample = \"Sample\"\n",
    "else:\n",
    "    sample = \"Full\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_ICD(probs, sort=False):\n",
    "    if probs == []:\n",
    "        return []\n",
    "    temp = pd.DataFrame(probs)    \n",
    "    \n",
    "    if sort:\n",
    "        temp = temp.sort_values(by='icd_linprob_mean', ascending=False)\n",
    "    \n",
    "    # display(temp)\n",
    "    return temp.icd.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cause_list'] = df[['cause1_icd10', 'cause2_icd10', 'cause3_icd10', 'cause4_icd10', 'cause5_icd10']].apply(lambda x: x.dropna().tolist(), axis=1)\n",
    "df['sanity'] = df.output_probs.apply(prob_ICD, sort=test_sort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT4, Sort: Highest Prob, Round 2\n",
      "Shape: (3317, 25)\n",
      "No mismatch found!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model: {model_name}, Sort: {sort_scheme}, {round_num}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "error_flag = False\n",
    "for idx, a in df[['cause_list', 'sanity']].iterrows():\n",
    "    if a.cause_list[:5] != a.sanity[:5]:\n",
    "        print(idx)\n",
    "        print(a.cause_list)\n",
    "        print(a.sanity)\n",
    "        error_flag = True\n",
    "        \n",
    "if error_flag:\n",
    "    print(\"Mismatch found!\")\n",
    "else:\n",
    "    print(\"No mismatch found!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[3280].output_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
