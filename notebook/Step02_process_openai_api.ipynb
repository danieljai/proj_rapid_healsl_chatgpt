{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis script loads the dataset compiled from the previous step and generate a responses from OpenAI\\'s API for each record.\\n\\nPseudo code\\n-----------\\n1. Load results storage as array.\\n    1.1 If result storage is does not exist, create an empty array.\\n2. For each row in the dataframe:\\n    2.1 Check if rowid is in the result storage.\\n        2.1.1 If exists: record, skip the row.\\n    2.2 Compose the two prompts and generate a response using the OpenAI API.\\n    2.3 Store the response and other relevant information in the result storage.\\n    2.4 Save the result storage to a file periodically.\\n3. Save the result storage one last time.\\n    3.1 If rows were skipped, print a warning message.\\n    3.2 save the skipped rows to a file.\\n\\n\\nDetails regarding #2.1.1 of the pseudo code:\\n--------------------------------------------\\n\\nSince responses are billed by token consumption, we want to avoid reprocessing the same record. Previously processed \\nare stored and saved using the unique identifier as key, and when the same storage file is loaded, the script checks\\nwhether the unique identifier is already in the storage.\\n\\nFor accounting purposes, we store any unique identifier that was skipped in a separate file. in #3.2.\\n\\n    \\nDetails regarding #2.2 of the pseudo code:\\n------------------------------------------\\n\\nWe utilize the OpenAI API\\'s Chat Completions to generate a response from the model. The parameters used are as follows:\\n\\n    message: This is input text to be processed by the API. It is composed of two text prompts: \\n        System Prompt: Provides the model context on its role and the expected output format. Same for all records. \\n            We used the following system prompt:\\n            \"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning \\n            ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. \\n            E.g. A00 If there are multiple ICD-10 codes, show one code per line\"\\n            \\n        User Prompt: Specific instructions and individual record data.\\n            We used the following user prompt:\\n            \"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 \\n            code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: \\n            {open_narrative}\"\\n            \\n            AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\\n            SEX_COD: replaced with sex_cod value from the questionnaire dataset.\\n            open_narrative: replaced with summary value from the open narrative dataset.\\n\\n    model: This parameter specifies the language model to be used. To strive for consistency and reproducibility of the \\n    results, we used specific versions of the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\\n    \\n    temperature: This parameter can be any value between 0 and 2. Low temperatures result in more deterministic responses, \\n    while high temperatures result in more random responses. We set this to 0.\\n\\n    lobprobs: This parameters controls whether the output includes the log probabilities of the tokens. We set it to True.\\n\\n    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\\n    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\\n    open_narrative: replaced with summary value from the open narrative dataset.\\n\\n\\nDetails regarding #2.3 of the pseudo code:\\n------------------------------------------\\n\\nBelow is the data structure of the output data:\\n\\n    record[uid] = {\\n        \\'uid\\': the unique identifier for the dataset  --> change to uid\\n        \\'rowid\\': original rowid,\\n        \\'param_model\\': model used\\n        \\'param_temperature\\': temperature used\\n        \\'param_logprobs\\': logprobs used\\n        \\'param_system_prompt\\': system prompt used\\n        \\'param_user_prompt\\': user prompt used\\n        # \\'output_msg\\': output text message from the API // drop\\n        # \\'output_logprobs\\': the log probabilities of the tokens in the output message // drop\\n        # \\'output_usage\\': token consumption // drop\\n        \\'output_timestamp\\': current timestamp\\n        # \\'other_columns\\': extra columns that were not recognized by the script. Can be useful for debugging or incorporating additional information.\\n        \\'output\\': serialized response from the Chat Completions API\\n        ... : passthough columns not required by the script but included in the original dataset\\n    }\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script loads the dataset compiled from the previous step and generate a responses from OpenAI's API for each record.\n",
    "\n",
    "Pseudo code\n",
    "-----------\n",
    "1. Load results storage as array.\n",
    "    1.1 If result storage is does not exist, create an empty array.\n",
    "2. For each row in the dataframe:\n",
    "    2.1 Check if rowid is in the result storage.\n",
    "        2.1.1 If exists: record, skip the row.\n",
    "    2.2 Compose the two prompts and generate a response using the OpenAI API.\n",
    "    2.3 Store the response and other relevant information in the result storage.\n",
    "    2.4 Save the result storage to a file periodically.\n",
    "3. Save the result storage one last time.\n",
    "    3.1 If rows were skipped, print a warning message.\n",
    "    3.2 save the skipped rows to a file.\n",
    "\n",
    "\n",
    "Details regarding #2.1.1 of the pseudo code:\n",
    "--------------------------------------------\n",
    "\n",
    "Since responses are billed by token consumption, we want to avoid reprocessing the same record. Previously processed \n",
    "are stored and saved using the unique identifier as key, and when the same storage file is loaded, the script checks\n",
    "whether the unique identifier is already in the storage.\n",
    "\n",
    "For accounting purposes, we store any unique identifier that was skipped in a separate file. in #3.2.\n",
    "\n",
    "    \n",
    "Details regarding #2.2 of the pseudo code:\n",
    "------------------------------------------\n",
    "\n",
    "We utilize the OpenAI API's Chat Completions to generate a response from the model. The parameters used are as follows:\n",
    "\n",
    "    message: This is input text to be processed by the API. It is composed of two text prompts: \n",
    "        System Prompt: Provides the model context on its role and the expected output format. Same for all records. \n",
    "            We used the following system prompt:\n",
    "            \"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning \n",
    "            ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. \n",
    "            E.g. A00 If there are multiple ICD-10 codes, show one code per line\"\n",
    "            \n",
    "        User Prompt: Specific instructions and individual record data.\n",
    "            We used the following user prompt:\n",
    "            \"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 \n",
    "            code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: \n",
    "            {open_narrative}\"\n",
    "            \n",
    "            AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "            SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "            open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "    model: This parameter specifies the language model to be used. To strive for consistency and reproducibility of the \n",
    "    results, we used specific versions of the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\n",
    "    \n",
    "    temperature: This parameter can be any value between 0 and 2. Low temperatures result in more deterministic responses, \n",
    "    while high temperatures result in more random responses. We set this to 0.\n",
    "\n",
    "    lobprobs: This parameters controls whether the output includes the log probabilities of the tokens. We set it to True.\n",
    "\n",
    "    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "    open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "\n",
    "Details regarding #2.3 of the pseudo code:\n",
    "------------------------------------------\n",
    "\n",
    "Below is the data structure of the output data:\n",
    "\n",
    "    record[uid] = {\n",
    "        'uid': the unique identifier for the dataset  --> change to uid\n",
    "        'rowid': original rowid,\n",
    "        'param_model': model used\n",
    "        'param_temperature': temperature used\n",
    "        'param_logprobs': logprobs used\n",
    "        'param_system_prompt': system prompt used\n",
    "        'param_user_prompt': user prompt used\n",
    "        # 'output_msg': output text message from the API // drop\n",
    "        # 'output_logprobs': the log probabilities of the tokens in the output message // drop\n",
    "        # 'output_usage': token consumption // drop\n",
    "        'output_timestamp': current timestamp\n",
    "        # 'other_columns': extra columns that were not recognized by the script. Can be useful for debugging or incorporating additional information.\n",
    "        'output': serialized response from the Chat Completions API\n",
    "        ... : passthough columns not required by the script but included in the original dataset\n",
    "    }\n",
    "    \n",
    "Duration\n",
    "- GPT4 took 686m 31s to process all data.\n",
    "- observed API stuck for more than 7 mins\n",
    "    \n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "import datetime\n",
    "import pytz\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "open_api_key = os.environ.get('OPEN_API_KEY')\n",
    "client = OpenAI(api_key=open_api_key)\n",
    "\n",
    "DEMO_MODE = False        # If set to True, the script will only process a subset of the data\n",
    "DEMO_RANDOM = False     # If set to True, the script will process a random subset of the data\n",
    "DEMO_SIZE_LIMIT = 10    # Size of the demo\n",
    "\n",
    "\n",
    "# Discard columns that are not recognized by the script in the output file.\n",
    "# Keep this to False if you want the output file to retain columns needed for post-processing purposes.\n",
    "DROP_EXCESS_COLUMNS = False\n",
    "\n",
    "INPUT_DATASET_FILE = \"healsl_dataset_all_240309_040141.csv\"     # Input file# Input datasets\n",
    "\n",
    "OUTPUT_DATA_FILE = \"all_data_gpt4_0313.json\"                         # Output file\n",
    "\n",
    "\n",
    "\n",
    "# Set the timezone to Eastern Time\n",
    "TIMEZONE = pytz.timezone('US/Eastern')\n",
    "\n",
    "WORDWRAP_WIDTH = 100\n",
    "\n",
    "# How often the temp storage is saved to disk\n",
    "SAVE_FREQ = 5\n",
    "\n",
    "# Models\n",
    "GPT4 = \"gpt-4-0613\"\n",
    "GPT3 = \"gpt-3.5-turbo-0125\"\n",
    "MODEL_NAME = GPT4\n",
    "TEMPERATURE = 0\n",
    "LOGPROBS = True\n",
    "\n",
    "\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning the most probable ICD-10 code for each death using verbal autopsy narratives. Return only the ICD-10 code without description. E.g. A00 \n",
    "If there are multiple ICD-10 codes, show one code per line.\"\"\"\n",
    "\n",
    "USR_PROMPT = \"\"\"Determine the underlying cause of death and provide the most probable ICD-10 code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_passed = True\n",
    "required_colnames = ['uid', 'rowid', 'age_value_death', 'age_unit_death',\n",
    "                     'open_narrative', 'sex_cod']\n",
    "\n",
    "# Load dataset\n",
    "merged_df = pd.read_csv(INPUT_DATASET_FILE)\n",
    "\n",
    "\n",
    "# Check all required columns are in the df\n",
    "for colnames in required_colnames:\n",
    "    if colnames not in merged_df.columns:\n",
    "        print(f\"Missing column \\\"{colnames}\\\"\")\n",
    "        dataset_passed = False\n",
    "\n",
    "if not dataset_passed:\n",
    "    print(\"Please ensure dataset has all the required columns.\")\n",
    "    raise ValueError(f\"Error: Missing columns required for processing.\")\n",
    "    \n",
    "# Get columns names that are not required\n",
    "extra_colnames = ['uid']\n",
    "extra_colnames += [colname for colname in merged_df.columns if colname not in required_colnames]\n",
    "\n",
    "# Transform non-required columns as dictionary in a new column\n",
    "# merged_df['model_residual_columns'] = merged_df[extra_colnames].apply(lambda x: x.to_dict(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When DEMO_MODE is set to True, process only a subset of the data\n",
    "if DEMO_MODE:\n",
    "    limit_records = int(DEMO_SIZE_LIMIT)\n",
    "    merged_df.sort_values(by='uid', inplace=True)\n",
    "    print(f\"DEMO MODE: Processing only {limit_records} records.\")\n",
    "    if DEMO_RANDOM:\n",
    "        merged_df = merged_df.sample(limit_records)\n",
    "    else:\n",
    "        merged_df = merged_df.head(limit_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Misc. functions\n",
    "\n",
    "# Function to get current time in string YYMMDD_HHMMSS format\n",
    "def get_current_str_time():\n",
    "    return datetime.datetime.now(tz=TIMEZONE).strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "# Used to convert ChatCompletion object to a dictionary, recursively\n",
    "def recursive_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursive_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_dict(v) for v in obj]\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return recursive_dict(obj.__dict__)\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Initialize the data storage dictionary\n",
    "\n",
    "def load_data(filename=OUTPUT_DATA_FILE):\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} found. Loading data...\")\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"{filename} not found. Initializing empty dictionary...\")\n",
    "        return {}\n",
    "\n",
    "def save_data(data, filename=OUTPUT_DATA_FILE):           \n",
    "    # Save data to a file   \n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Send a message to the chatbot\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # max_tokens=500,\n",
    "    temperature=0,\n",
    "    # stop=None,\n",
    "    # seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        # \"response_format\": { \"type\": \"json_object\" },\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        # \"stop\": stop,\n",
    "        # \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data_gpt4_0313.json found. Loading data...\n",
      "\n",
      "\n",
      "\n",
      "Saving index: 1025     Processing: 14000757     Rows skipped: 91\n",
      "\n",
      "Saving index: 1375     Processing: 14003545     Rows skipped: 92\n",
      "\n",
      "Saving index: 2345     Processing: 14001953     Rows skipped: 93\n",
      "\n",
      "Saving index: 3710     Processing: 14000093     Rows skipped: 94\n",
      "\n",
      "Saving index: 4915     Processing: 14001076     Rows skipped: 95\n",
      "\n",
      "Saving index: 5700     Processing: 14006579     Rows skipped: 96\n",
      "\n",
      "Saving index: 5960     Processing: 14001581     Rows skipped: 97\n",
      "\n",
      "Saving index: 6990     Processing: 14000596     Rows skipped: 98\n",
      "\n",
      "Saving index: 7320     Processing: 14004826     Rows skipped: 99\n",
      "\n",
      "Saving index: 8330     Processing: 14001654     Rows skipped: 100\n",
      "\n",
      "Saving index: 11886    Processing: 24001069     Rows skipped: 101\n",
      "Data saved successfully. Processing Complete.\n",
      "101 rows skipped. Check skipped_rows for details.\n"
     ]
    }
   ],
   "source": [
    "# Load existing data or initialize an empty dictionary\n",
    "data_storage = load_data()\n",
    "skipped_rows = []\n",
    "repeated_skips = False\n",
    "print()\n",
    "\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Access the values of each column in the current row\n",
    "    # hijacking row \n",
    "    # row = merged_df[merged_df['rowid'] == 14005966].iloc[0]\n",
    "    \n",
    "    uid = row['uid']    \n",
    "    \n",
    "    # Check if rowid already processed. Testing both because json changes int keys to str    \n",
    "    if (uid) in data_storage or str(uid) in data_storage:\n",
    "        # if repeated_skips:\n",
    "        #     print(\"\\r\", end='', flush=True)\n",
    "        # print(f\"Skipping index {index}, row {uid} - Already processed.\", end='', flush=True)\n",
    "        repeated_skips = True\n",
    "        skipped_rows.append(uid)\n",
    "        continue\n",
    "\n",
    "    rowid = row['rowid']\n",
    "    narrative = row['open_narrative']\n",
    "    sex_cod = row['sex_cod']\n",
    "    age_value_death = row['age_value_death']\n",
    "    age_unit_death = row['age_unit_death']\n",
    "    other_columns = row[extra_colnames].to_dict()\n",
    "    \n",
    "    prompt = USR_PROMPT\n",
    "    prompt = prompt.replace('AGE_VALUE_DEATH', str(age_value_death))\n",
    "    prompt = prompt.replace('AGE_UNIT_DEATH', age_unit_death.lower())\n",
    "    prompt = prompt.replace('SEX_COD', str(sex_cod).lower())\n",
    "    prompt = prompt.format(open_narrative=narrative)\n",
    "    \n",
    "    # print(\"Prompt:\")    \n",
    "    # print(textwrap.fill(prompt, width=WORDWRAP_WIDTH))\n",
    "    # print()\n",
    "    \n",
    "    # for a in range(5):\n",
    "    completion = get_completion(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] ,\n",
    "        model=MODEL_NAME,\n",
    "        logprobs=LOGPROBS,\n",
    "        temperature=TEMPERATURE,\n",
    "        # top_logprobs=2,\n",
    "    )\n",
    "    \n",
    "    # print(completion.choices[0].message)\n",
    "    \n",
    "    # for token in completion.choices[0].logprobs.content:\n",
    "    #     print(f\"{repr(str(token.token)).ljust(15)}  {str(token.logprob).ljust(20)} {np.round(np.exp(token.logprob)*100,2)}%\")\n",
    "        \n",
    "    output_msg = completion.choices[0].message.content\n",
    "    logprob_data = [(token.token, float(token.logprob)) for token in completion.choices[0].logprobs.content]\n",
    "    usage_data = list(completion.usage)    \n",
    "    current_time = datetime.datetime.now(tz=TIMEZONE).isoformat()\n",
    "       \n",
    "    data_storage[str(uid)] = {\n",
    "        'uid': uid,               # 'uid' is the unique identifier for the dataset\n",
    "        'rowid': rowid,\n",
    "        'param_model': MODEL_NAME,\n",
    "        'param_temperature': 0,\n",
    "        'param_logprobs': True,\n",
    "        'param_system_prompt': SYS_PROMPT,\n",
    "        'param_user_prompt': prompt,\n",
    "        # 'output_msg': output_msg,\n",
    "        # 'logprobs': logprob_data,\n",
    "        # 'usage': usage_data,\n",
    "        'timestamp': current_time,\n",
    "        # 'other_columns': other_columns,\n",
    "        'output': recursive_dict(completion),\n",
    "    }\n",
    "\n",
    "    if not DROP_EXCESS_COLUMNS:\n",
    "        # Append columns not required by the script but exists on the original dataset\n",
    "        data_storage[str(uid)].update(other_columns)\n",
    "\n",
    "    # Save data periodically (you can adjust the frequency based on your needs)    \n",
    "    if index % SAVE_FREQ == 0 and index > 0:\n",
    "        if repeated_skips:\n",
    "            print(\"\\n\", flush=True)\n",
    "        repeated_skips = False\n",
    "        \n",
    "        save_data(data_storage)\n",
    "        print(f\"Saving index: {str(index).ljust(8)} Processing: {str(uid).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "        # break\n",
    "    \n",
    "try:\n",
    "    save_data(data_storage)\n",
    "    print(f\"Saving index: {str(index).ljust(8)} Processing: {str(uid).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "    print(\"\\nData saved successfully. Processing Complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "if len(skipped_rows) > 0:\n",
    "    print(f\"{len(skipped_rows)} rows skipped. Check skipped_rows for details.\")\n",
    "    \n",
    "    # Write skipped rows to a file\n",
    "    with open(f\"Step02_process_openai_report_{get_current_str_time()}.txt\", \"w\") as file:\n",
    "        file.write(f\"The follow rows are skipped because they were already processed.\\n\")\n",
    "        for item in skipped_rows:        \n",
    "            file.write(f\"{str(item)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nan'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sex_cod)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
