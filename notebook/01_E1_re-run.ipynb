{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe HEALSL project provides verbal autopsy data into multiple different files. Round one and round two; adult, child, and neonate; and questionnaire, \\nage, and open narrative are all separate files. This script aims to combine these files into a single dataset (dataframe) to simplify processing.\\n\\nWe extract the deceased\\'s sex from the questionnaire dataset, the deceased\\'s age from the age dataset, and the open narrative recorded from the\\nverbal autopsy from the narrative dataset. Then, we combine the extracted features (columns) using their row id as the key.\\n\\nWe utilize the OpenAI API\\'s Chat Completions to generate a response for each verbal autopsy record. Several parameters were used for this project:\\nmessage: the input text to be processed by the API. It consists of the combination of two text prompts: the system prompt, which is the same for \\nall requests provides the model with some context of its role and objective, and the user prompt, which concatenates more specific instructions\\nregarding the output response, along with the data from the dataframe.\\n\\nmodel: this parameter specifies the language model to be used. To strive for consistency and reproducibility of the results, we used specific versions\\nof the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\\n\\ntemperature: this parameter can be any value between 0 and 2. It controls the randomness of the output response. Low temperatures result in more \\ndeterministic responses, while high temperatures result in more random responses. We used a temperature of 0 to ensure the responses were as \\ndeterministic as possible.\\n\\nlobprobs: this parameters controls whether the output includes the log probabilities of the tokens. We set it to True to provide more parametric \\ninformation about the output.\\n\\nThe prompts are as follows:\\n\\nSystem prompt:\\n\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning ICD-10 codes for deaths using verbal autopsy \\nnarratives. Return only the ICD-10 code without description. E.g. A00 \\nIf there are multiple ICD-10 codes, show one code per line\"\\n\\nUser prompt:\\n\"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 code for a verbal autopsy narrative of a\\nAGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\\n\\n    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\\n    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\\n    open_narrative: replaced with summary value from the open narrative dataset.\\n\\nThe response from the API is then dissected to extract relevant information in plain text, such as the response text, the log probabilities, \\nalong with other accounting information such as rowids, models used, timestamps, token consumption, into an array and exported into file.\\n        \\n\\n1. Load results storage as array.\\n    If result storage is does not exist, create an empty array.\\n2. For each row in the dataframe:\\n    Check if rowid is in the result storage.\\n        If rowid is in the result storage, skip the row.\\n    Compose the two prompts and generate a response using the OpenAI API.\\n    Store the response and other relevant information in the result storage.\\n    Save the result storage to a file periodically.\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script loads the dataset compiled from the previous step and generate a responses from OpenAI's API for each record.\n",
    "\n",
    "Pseudo code\n",
    "-----------\n",
    "1. Load results storage as array.\n",
    "    1.1 If result storage is does not exist, create an empty array.\n",
    "2. For each row in the dataframe:\n",
    "    2.1 Check if rowid is in the result storage.\n",
    "        2.1.1 If exists: record, skip the row.\n",
    "    2.2 Compose the two prompts and generate a response using the OpenAI API.\n",
    "    2.3 Store the response and other relevant information in the result storage.\n",
    "    2.4 Save the result storage to a file periodically.\n",
    "3. Save the result storage one last time.\n",
    "    3.1 If rows were skipped, print a warning message.\n",
    "    3.2 save the skipped rows to a file.\n",
    "\n",
    "\n",
    "Details regarding #2.1.1 of the pseudo code:\n",
    "--------------------------------------------\n",
    "\n",
    "Since responses are billed by token consumption, we want to avoid reprocessing the same record. Previously processed \n",
    "are stored and saved using the unique identifier as key, and when the same storage file is loaded, the script checks\n",
    "whether the unique identifier is already in the storage.\n",
    "\n",
    "For accounting purposes, we store any unique identifier that was skipped in a separate file. in #3.2.\n",
    "\n",
    "    \n",
    "Details regarding #2.2 of the pseudo code:\n",
    "------------------------------------------\n",
    "\n",
    "We utilize the OpenAI API's Chat Completions to generate a response from the model. The parameters used are as follows:\n",
    "\n",
    "    message: This is input text to be processed by the API. It is composed of two text prompts: \n",
    "        System Prompt: Provides the model context on its role and the expected output format. Same for all records. \n",
    "            We used the following system prompt:\n",
    "            \"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning \n",
    "            ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. \n",
    "            E.g. A00 If there are multiple ICD-10 codes, show one code per line\"\n",
    "            \n",
    "        User Prompt: Specific instructions and individual record data.\n",
    "            We used the following user prompt:\n",
    "            \"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 \n",
    "            code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: \n",
    "            {open_narrative}\"\n",
    "            \n",
    "            AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "            SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "            open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "    model: This parameter specifies the language model to be used. To strive for consistency and reproducibility of the \n",
    "    results, we used specific versions of the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\n",
    "    \n",
    "    temperature: This parameter can be any value between 0 and 2. Low temperatures result in more deterministic responses, \n",
    "    while high temperatures result in more random responses. We set this to 0.\n",
    "\n",
    "    lobprobs: This parameters controls whether the output includes the log probabilities of the tokens. We set it to True.\n",
    "\n",
    "    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "    open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "\n",
    "Details regarding #2.3 of the pseudo code:\n",
    "------------------------------------------\n",
    "\n",
    "Below is the data structure of the output data:\n",
    "\n",
    "    record[u_id] = {\n",
    "        'u_id': the unique identifier for the dataset\n",
    "        'rowid': original rowid,\n",
    "        'model': model used\n",
    "        'system_prompt': system prompt used\n",
    "        'user_prompt': user prompt used\n",
    "        'output_msg': output text message from the API\n",
    "        'logprobs': the log probabilities of the tokens in the output message\n",
    "        'usage': token consumption\n",
    "        'timestamp': current timestamp\n",
    "        'other_columns': extra columns that were not recognized by the script. Can be useful for debugging or incorporating additional information.\n",
    "        'raw': serialized response from the Chat Completions API\n",
    "    }\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "import datetime\n",
    "import pytz\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "open_api_key = os.environ.get('OPEN_API_KEY')\n",
    "client = OpenAI(api_key=open_api_key)\n",
    "\n",
    "# Input datasets\n",
    "INPUT_DATASET_FILE = \"output.csv\"\n",
    "\n",
    "# Output file\n",
    "OUTPUT_DATA_FILE = \"testing_response.json\"\n",
    "\n",
    "\n",
    "WORDWRAP_WIDTH = 100\n",
    "SAVE_FREQ = 5\n",
    "\n",
    "# Models\n",
    "GPT4 = \"gpt-4-0613\"\n",
    "GPT3 = \"gpt-3.5-turbo-0125\"\n",
    "MODEL_NAME = GPT3\n",
    "\n",
    "# Set the timezone to Eastern Time\n",
    "TIMEZONE = pytz.timezone('US/Eastern')\n",
    "\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. E.g. A00 \n",
    "If there are multiple ICD-10 codes, show one code per line.\"\"\"\n",
    "\n",
    "USR_PROMPT = \"\"\"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_passed = True\n",
    "required_colnames = ['u_id', 'rowid', 'age_value_death', 'age_unit_death',\n",
    "                     'open_narrative', 'sex_cod']\n",
    "\n",
    "# Load dataset\n",
    "merged_df = pd.read_csv(INPUT_DATASET_FILE)\n",
    "\n",
    "# TEST: making up a new column\n",
    "merged_df = merged_df.assign(other_2=\"extra stuff\")\n",
    "\n",
    "# Check all required columns are in the df\n",
    "for colnames in required_colnames:\n",
    "    if colnames not in merged_df.columns:\n",
    "        print(f\"Missing column \\\"{colnames}\\\"\")\n",
    "        dataset_passed = False\n",
    "\n",
    "if not dataset_passed:\n",
    "    print(\"Please ensure dataset has all the required columns.\")\n",
    "    raise ValueError(f\"Error: Missing columns required for processing.\")\n",
    "    \n",
    "# Get columns names that are not required\n",
    "extra_colnames = [colname for colname in merged_df.columns if colname not in required_colnames]\n",
    "\n",
    "# Transform non-required columns as dictionary in a new column\n",
    "merged_df['model_residual_columns'] = merged_df[extra_colnames].apply(lambda x: x.to_dict(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_id</th>\n",
       "      <th>rowid</th>\n",
       "      <th>open_narrative</th>\n",
       "      <th>sex_cod</th>\n",
       "      <th>age_value_death</th>\n",
       "      <th>age_unit_death</th>\n",
       "      <th>group</th>\n",
       "      <th>other_2</th>\n",
       "      <th>model_residual_columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>14008167_5</td>\n",
       "      <td>14008167</td>\n",
       "      <td>According to the respondent the deceased was a...</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>Years</td>\n",
       "      <td>child_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "      <td>{'group': 'child_rd1', 'other_2': 'extra stuff'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>24001661_4</td>\n",
       "      <td>24001661</td>\n",
       "      <td>According to the respondent, the deceased was ...</td>\n",
       "      <td>Female</td>\n",
       "      <td>7</td>\n",
       "      <td>Months</td>\n",
       "      <td>child_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "      <td>{'group': 'child_rd2', 'other_2': 'extra stuff'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>14004923_8</td>\n",
       "      <td>14004923</td>\n",
       "      <td>As per respondent, the deceased was a 0 day ol...</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Days</td>\n",
       "      <td>neo_rd1</td>\n",
       "      <td>extra stuff</td>\n",
       "      <td>{'group': 'neo_rd1', 'other_2': 'extra stuff'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>24001844_8</td>\n",
       "      <td>24001844</td>\n",
       "      <td>As per respondents the deceased was a one year...</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>Years</td>\n",
       "      <td>child_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "      <td>{'group': 'child_rd2', 'other_2': 'extra stuff'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>24002976_0</td>\n",
       "      <td>24002976</td>\n",
       "      <td>As per the respondent the deceased was a 40 ye...</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>Years</td>\n",
       "      <td>adult_rd2</td>\n",
       "      <td>extra stuff</td>\n",
       "      <td>{'group': 'adult_rd2', 'other_2': 'extra stuff'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           u_id     rowid                                     open_narrative  \\\n",
       "560  14008167_5  14008167  According to the respondent the deceased was a...   \n",
       "494  24001661_4  24001661  According to the respondent, the deceased was ...   \n",
       "871  14004923_8  14004923  As per respondent, the deceased was a 0 day ol...   \n",
       "892  24001844_8  24001844  As per respondents the deceased was a one year...   \n",
       "86   24002976_0  24002976  As per the respondent the deceased was a 40 ye...   \n",
       "\n",
       "    sex_cod  age_value_death age_unit_death      group      other_2  \\\n",
       "560    Male                2          Years  child_rd1  extra stuff   \n",
       "494  Female                7         Months  child_rd2  extra stuff   \n",
       "871  Female                0           Days    neo_rd1  extra stuff   \n",
       "892    Male                1          Years  child_rd2  extra stuff   \n",
       "86     Male               40          Years  adult_rd2  extra stuff   \n",
       "\n",
       "                               model_residual_columns  \n",
       "560  {'group': 'child_rd1', 'other_2': 'extra stuff'}  \n",
       "494  {'group': 'child_rd2', 'other_2': 'extra stuff'}  \n",
       "871    {'group': 'neo_rd1', 'other_2': 'extra stuff'}  \n",
       "892  {'group': 'child_rd2', 'other_2': 'extra stuff'}  \n",
       "86   {'group': 'adult_rd2', 'other_2': 'extra stuff'}  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ***** REMOVE THIS WHEN DONE TESTING *****\n",
    "merged_df = merged_df.sample(5)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Misc. functions\n",
    "\n",
    "# Function to get current time in string YYMMDD_HHMMSS format\n",
    "def get_current_str_time():\n",
    "    return datetime.datetime.now(tz=TIMEZONE).strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "# Used to convert ChatCompletion object to a dictionary, recursively\n",
    "def recursive_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursive_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_dict(v) for v in obj]\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return recursive_dict(obj.__dict__)\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Initialize the data storage dictionary\n",
    "\n",
    "def load_data(filename=OUTPUT_DATA_FILE):\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} found. Loading data...\")\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"{filename} not found. Initializing empty dictionary...\")\n",
    "        return {}\n",
    "\n",
    "def save_data(data, filename=OUTPUT_DATA_FILE):           \n",
    "    # Save data to a file   \n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Send a message to the chatbot\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # max_tokens=500,\n",
    "    temperature=0,\n",
    "    # stop=None,\n",
    "    # seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        # \"response_format\": { \"type\": \"json_object\" },\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        # \"stop\": stop,\n",
    "        # \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing_response.json not found. Initializing empty dictionary...\n",
      "\n",
      "Saving index: 86       Processing: 24002976_0   Rows skipped: 0\n",
      "Data saved successfully. Processing Complete.\n"
     ]
    }
   ],
   "source": [
    "# Load existing data or initialize an empty dictionary\n",
    "data_storage = load_data()\n",
    "skipped_rows = []\n",
    "repeated_skips = False\n",
    "print()\n",
    "\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Access the values of each column in the current row\n",
    "    # hijacking row \n",
    "    # row = merged_df[merged_df['rowid'] == 14005966].iloc[0]\n",
    "    \n",
    "    u_id = row['u_id']    \n",
    "    \n",
    "    # Check if rowid already processed. Testing both because json changes int keys to str    \n",
    "    if (u_id) in data_storage or str(u_id) in data_storage:\n",
    "        # if repeated_skips:\n",
    "        #     print(\"\\r\", end='', flush=True)\n",
    "        # print(f\"Skipping index {index}, row {u_id} - Already processed.\", end='', flush=True)\n",
    "        repeated_skips = True\n",
    "        skipped_rows.append(u_id)\n",
    "        continue\n",
    "\n",
    "    rowid = row['rowid']\n",
    "    narrative = row['open_narrative']\n",
    "    sex_cod = row['sex_cod']\n",
    "    age_value_death = row['age_value_death']\n",
    "    age_unit_death = row['age_unit_death']\n",
    "    other_columns = row['model_residual_columns']\n",
    "    \n",
    "    prompt = USR_PROMPT\n",
    "    prompt = prompt.replace('AGE_VALUE_DEATH', str(age_value_death))\n",
    "    prompt = prompt.replace('AGE_UNIT_DEATH', age_unit_death.lower())\n",
    "    prompt = prompt.replace('SEX_COD', sex_cod.lower())\n",
    "    prompt = prompt.format(open_narrative=narrative)\n",
    "    \n",
    "    # print(\"Prompt:\")    \n",
    "    # print(textwrap.fill(prompt, width=WORDWRAP_WIDTH))\n",
    "    # print()\n",
    "    \n",
    "    # for a in range(5):\n",
    "    completion = get_completion(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] ,\n",
    "        model=MODEL_NAME,\n",
    "        logprobs=True,\n",
    "        # top_logprobs=2,\n",
    "    )\n",
    "    \n",
    "    # print(completion.choices[0].message)\n",
    "    \n",
    "    # for token in completion.choices[0].logprobs.content:\n",
    "    #     print(f\"{repr(str(token.token)).ljust(15)}  {str(token.logprob).ljust(20)} {np.round(np.exp(token.logprob)*100,2)}%\")\n",
    "        \n",
    "    output_msg = completion.choices[0].message.content\n",
    "    logprob_data = [(token.token, float(token.logprob)) for token in completion.choices[0].logprobs.content]\n",
    "    usage_data = list(completion.usage)    \n",
    "    current_time = datetime.datetime.now(tz=TIMEZONE).isoformat()\n",
    "       \n",
    "    data_storage[str(u_id)] = {\n",
    "        'u_id': u_id,               # 'u_id' is the unique identifier for the dataset\n",
    "        'rowid': rowid,\n",
    "        'model': MODEL_NAME,\n",
    "        'system_prompt': SYS_PROMPT,\n",
    "        'user_prompt': prompt,\n",
    "        'output_msg': output_msg,\n",
    "        'logprobs': logprob_data,\n",
    "        'usage': usage_data,\n",
    "        'timestamp': current_time,\n",
    "        'other_columns': other_columns,\n",
    "        'raw': recursive_dict(completion),\n",
    "    }\n",
    "\n",
    "    # Save data periodically (you can adjust the frequency based on your needs)    \n",
    "    if index % SAVE_FREQ == 0 and index > 0:\n",
    "        if repeated_skips:\n",
    "            print(\"\\n\", flush=True)\n",
    "        repeated_skips = False\n",
    "        \n",
    "        save_data(data_storage)\n",
    "        print(f\"Saving index: {str(index).ljust(8)} Processing: {str(u_id).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "        # break\n",
    "    \n",
    "try:\n",
    "    save_data(data_storage)\n",
    "    print(f\"Saving index: {str(index).ljust(8)} Processing: {str(u_id).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "    print(\"\\nData saved successfully. Processing Complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "if len(skipped_rows) > 0:\n",
    "    print(f\"{len(skipped_rows)} rows skipped. Check skipped_rows for details.\")\n",
    "    \n",
    "    # Write skipped rows to a file\n",
    "    with open(f\"response_report_{get_current_str_time()}.txt\", \"w\") as file:\n",
    "        file.write(f\"The follow rows are skipped because they were already processed.\\n\")\n",
    "        for item in skipped_rows:        \n",
    "            file.write(f\"{str(item)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
