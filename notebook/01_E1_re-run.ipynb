{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The HEALSL project provides verbal autopsy data into multiple different files. Round one and round two; adult, child, and neonate; and questionnaire, \n",
    "age, and open narrative are all separate files. This script aims to combine these files into a single dataset (dataframe) to simplify processing.\n",
    "\n",
    "We extract the deceased's sex from the questionnaire dataset, the deceased's age from the age dataset, and the open narrative recorded from the\n",
    "verbal autopsy from the narrative dataset. Then, we combine the extracted features (columns) using their row id as the key.\n",
    "\n",
    "We utilize the OpenAI API's Chat Completions to generate a response for each verbal autopsy record. Several parameters were used for this project:\n",
    "message: the input text to be processed by the API. It consists of the combination of two text prompts: the system prompt, which is the same for \n",
    "all requests provides the model with some context of its role and objective, and the user prompt, which concatenates more specific instructions\n",
    "regarding the output response, along with the data from the dataframe.\n",
    "\n",
    "model: this parameter specifies the language model to be used. To strive for consistency and reproducibility of the results, we used specific versions\n",
    "of the GPT-3 and GPT-4 models; gpt-3.5-turbo-0125 and gpt-4-0613, respectively.\n",
    "\n",
    "temperature: this parameter can be any value between 0 and 2. It controls the randomness of the output response. Low temperatures result in more \n",
    "deterministic responses, while high temperatures result in more random responses. We used a temperature of 0 to ensure the responses were as \n",
    "deterministic as possible.\n",
    "\n",
    "lobprobs: this parameters controls whether the output includes the log probabilities of the tokens. We set it to True to provide more parametric \n",
    "information about the output.\n",
    "\n",
    "The prompts are as follows:\n",
    "\n",
    "System prompt:\n",
    "\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning ICD-10 codes for deaths using verbal autopsy \n",
    "narratives. Return only the ICD-10 code without description. E.g. A00 \n",
    "If there are multiple ICD-10 codes, show one code per line\"\n",
    "\n",
    "User prompt:\n",
    "\"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 code for a verbal autopsy narrative of a\n",
    "AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\n",
    "\n",
    "    AGE_VALUE_DEATH and AGE_UNIT_DEATH: replaced with age_value_death and age_unit_death values from the age dataset.\n",
    "    SEX_COD: replaced with sex_cod value from the questionnaire dataset.\n",
    "    open_narrative: replaced with summary value from the open narrative dataset.\n",
    "\n",
    "The response from the API is then dissected to extract relevant information in plain text, such as the response text, the log probabilities, \n",
    "along with other accounting information such as rowids, models used, timestamps, token consumption, into an array and exported into file.\n",
    "        \n",
    "\n",
    "1. Load results storage as array.\n",
    "    If result storage is does not exist, create an empty array.\n",
    "2. For each row in the dataframe:\n",
    "    Check if rowid is in the result storage.\n",
    "        If rowid is in the result storage, skip the row.\n",
    "    Compose the two prompts and generate a response using the OpenAI API.\n",
    "    Store the response and other relevant information in the result storage.\n",
    "    Save the result storage to a file periodically.\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "import datetime\n",
    "import pytz\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "open_api_key = os.environ.get('OPEN_API_KEY')\n",
    "client = OpenAI(api_key=open_api_key)\n",
    "\n",
    "# Input datasets\n",
    "INPUT_DATASET_FILE = \"output.csv\"\n",
    "\n",
    "# Output file\n",
    "OUTPUT_DATA_FILE = \"testing_response.json\"\n",
    "\n",
    "\n",
    "WORDWRAP_WIDTH = 100\n",
    "SAVE_FREQ = 5\n",
    "\n",
    "# Models\n",
    "GPT4 = \"gpt-4-0613\"\n",
    "GPT3 = \"gpt-3.5-turbo-0125\"\n",
    "MODEL_NAME = GPT3\n",
    "\n",
    "# Set the timezone to Eastern Time\n",
    "TIMEZONE = pytz.timezone('US/Eastern')\n",
    "\n",
    "\n",
    "SYS_PROMPT = \"\"\"You are a physician with expertise in determining underlying causes of death in Sierra Leone by assigning ICD-10 codes for deaths using verbal autopsy narratives. Return only the ICD-10 code without description. E.g. A00 \n",
    "If there are multiple ICD-10 codes, show one code per line.\"\"\"\n",
    "\n",
    "USR_PROMPT = \"\"\"With the highest certainty, determine the underlying cause of death and provide the most accurate ICD-10 code for a verbal autopsy narrative of a AGE_VALUE_DEATH AGE_UNIT_DEATH old SEX_COD death in Sierra Leone: {open_narrative}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_passed = True\n",
    "required_colnames = ['u_id', 'rowid', 'age_value_death', 'age_unit_death',\n",
    "                     'open_narrative', 'sex_cod']\n",
    "\n",
    "# Load dataset\n",
    "merged_df = pd.read_csv(INPUT_DATASET_FILE)\n",
    "\n",
    "# TEST: making up a new column\n",
    "merged_df = merged_df.assign(other_2=\"extra stuff\")\n",
    "\n",
    "# Check all required columns are in the df\n",
    "for colnames in required_colnames:\n",
    "    if colnames not in merged_df.columns:\n",
    "        print(f\"Missing column \\\"{colnames}\\\"\")\n",
    "        dataset_passed = False\n",
    "\n",
    "if not dataset_passed:\n",
    "    print(\"Please ensure dataset has all the required columns.\")\n",
    "    raise ValueError(f\"Error: Missing columns required for processing.\")\n",
    "    \n",
    "# Get columns names that are not required\n",
    "extra_colnames = [colname for colname in merged_df.columns if colname not in required_colnames]\n",
    "\n",
    "# Transform non-required columns as dictionary in a new column\n",
    "merged_df['model_residual_columns'] = merged_df[extra_colnames].apply(lambda x: x.to_dict(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** REMOVE THIS WHEN DONE TESTING *****\n",
    "merged_df = merged_df.sample(5)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Misc. functions\n",
    "\n",
    "# Function to get current time in string YYMMDD_HHMMSS format\n",
    "def get_current_str_time():\n",
    "    return datetime.datetime.now(tz=TIMEZONE).strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "# Used to convert ChatCompletion object to a dictionary, recursively\n",
    "def recursive_dict(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: recursive_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_dict(v) for v in obj]\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return recursive_dict(obj.__dict__)\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Initialize the data storage dictionary\n",
    "\n",
    "def load_data(filename=OUTPUT_DATA_FILE):\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} found. Loading data...\")\n",
    "        with open(filename, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"{filename} not found. Initializing empty dictionary...\")\n",
    "        return {}\n",
    "\n",
    "def save_data(data, filename=OUTPUT_DATA_FILE):           \n",
    "    # Save data to a file   \n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F(x): Send a message to the chatbot\n",
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # model: str = \"gpt-3.5-turbo-0125\",\n",
    "    # max_tokens=500,\n",
    "    temperature=0,\n",
    "    # stop=None,\n",
    "    # seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        # \"response_format\": { \"type\": \"json_object\" },\n",
    "        \"messages\": messages,\n",
    "        # \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        # \"stop\": stop,\n",
    "        # \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing data or initialize an empty dictionary\n",
    "data_storage = load_data()\n",
    "skipped_rows = []\n",
    "repeated_skips = False\n",
    "print()\n",
    "\n",
    "\n",
    "for index, row in merged_df.iterrows():\n",
    "    # Access the values of each column in the current row\n",
    "    # hijacking row \n",
    "    # row = merged_df[merged_df['rowid'] == 14005966].iloc[0]\n",
    "    \n",
    "    u_id = row['u_id']    \n",
    "    \n",
    "    # Check if rowid already processed. Testing both because json changes int keys to str    \n",
    "    if (u_id) in data_storage or str(u_id) in data_storage:\n",
    "        # if repeated_skips:\n",
    "        #     print(\"\\r\", end='', flush=True)\n",
    "        # print(f\"Skipping index {index}, row {u_id} - Already processed.\", end='', flush=True)\n",
    "        repeated_skips = True\n",
    "        skipped_rows.append(u_id)\n",
    "        continue\n",
    "\n",
    "    rowid = row['rowid']\n",
    "    narrative = row['open_narrative']\n",
    "    sex_cod = row['sex_cod']\n",
    "    age_value_death = row['age_value_death']\n",
    "    age_unit_death = row['age_unit_death']\n",
    "    other_columns = row['model_residual_columns']\n",
    "    \n",
    "    prompt = USR_PROMPT\n",
    "    prompt = prompt.replace('AGE_VALUE_DEATH', str(age_value_death))\n",
    "    prompt = prompt.replace('AGE_UNIT_DEATH', age_unit_death.lower())\n",
    "    prompt = prompt.replace('SEX_COD', sex_cod.lower())\n",
    "    prompt = prompt.format(open_narrative=narrative)\n",
    "    \n",
    "    # print(\"Prompt:\")    \n",
    "    # print(textwrap.fill(prompt, width=WORDWRAP_WIDTH))\n",
    "    # print()\n",
    "    \n",
    "    # for a in range(5):\n",
    "    completion = get_completion(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ] ,\n",
    "        model=MODEL_NAME,\n",
    "        logprobs=True,\n",
    "        # top_logprobs=2,\n",
    "    )\n",
    "    \n",
    "    # print(completion.choices[0].message)\n",
    "    \n",
    "    # for token in completion.choices[0].logprobs.content:\n",
    "    #     print(f\"{repr(str(token.token)).ljust(15)}  {str(token.logprob).ljust(20)} {np.round(np.exp(token.logprob)*100,2)}%\")\n",
    "        \n",
    "    output_msg = completion.choices[0].message.content\n",
    "    logprob_data = [(token.token, float(token.logprob)) for token in completion.choices[0].logprobs.content]\n",
    "    usage_data = list(completion.usage)    \n",
    "    current_time = datetime.datetime.now(tz=TIMEZONE).isoformat()\n",
    "       \n",
    "    data_storage[str(u_id)] = {\n",
    "        'u_id': u_id,               # 'u_id' is the unique identifier for the dataset\n",
    "        'rowid': rowid,\n",
    "        'model': MODEL_NAME,\n",
    "        'system_prompt': SYS_PROMPT,\n",
    "        'user_prompt': prompt,\n",
    "        'output_msg': output_msg,\n",
    "        'logprobs': logprob_data,\n",
    "        'usage': usage_data,\n",
    "        'timestamp': current_time,\n",
    "        'other_columns': other_columns,\n",
    "        'raw': recursive_dict(completion),\n",
    "    }\n",
    "\n",
    "    # Save data periodically (you can adjust the frequency based on your needs)    \n",
    "    if index % SAVE_FREQ == 0 and index > 0:\n",
    "        if repeated_skips:\n",
    "            print(\"\\n\", flush=True)\n",
    "        repeated_skips = False\n",
    "        \n",
    "        save_data(data_storage)\n",
    "        print(f\"Saving index: {str(index).ljust(8)} Processing: {str(u_id).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "        # break\n",
    "    \n",
    "try:\n",
    "    save_data(data_storage)\n",
    "    print(f\"Saving index: {str(index).ljust(8)} Processing: {str(u_id).ljust(12)} Rows skipped: {len(skipped_rows)}\", sep=' ', end='\\r', flush=True)\n",
    "    print(\"\\nData saved successfully. Processing Complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "if len(skipped_rows) > 0:\n",
    "    print(f\"{len(skipped_rows)} rows skipped. Check skipped_rows for details.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"response_report_{get_current_str_time()}.txt\", \"w\") as file:\n",
    "    file.write(f\"The follow rows are skipped because they were already processed.\\n\")\n",
    "    for item in skipped_rows:        \n",
    "        file.write(f\"{str(item)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
